# conductor (DNAnexus Platform App)

DNAnexus app for automating end to end analysis of samples through workflows.

## What does this app do?

Overview of workflow for example assays:

<p align="center">
    <img style="padding-top:20px" src="images/workflow.png">
</p>


Workflow for triggering analysis of each assay for given set of samples:

<p align="center">
    <img style="padding-top:20px" src="images/run_workflows_diagram.png">
</p>


## What are typical use cases for this app?
Automating analysis for given samples from a config file definition. This can either be as an app triggered at the end of [dx-streaming-upload](dx-streaming-upload-url) or run as a stand-alone app with the required inputs.


## What data are required for this app to run?

- high level config file (`.tsv`) - maps sample naming to assay config file to use
- low level config file (`.json`) - full definition for running apps/workflows for an assay
- eggd_conductor config file (`.cfg`) - config file containing app ID for bcl2fastq, slack API token and DNAnexus auth token
- samplesheet (optional) - used to get sample names
- sentinel record file - generated by dx-streaming-upload, used to get sample sheet for run

## Config file design

The app is built to rely on 3 config files:

- a config file to store auth tokens
- a high level one that maps sample naming to assay config
- an assay specific config file that specifies all aspects of calling the required workflows and applets

### Auth token config

This currently must contain the following:

- `BCL2FASTQ_APP_ID=`: app ID of bcl2fastq for demultiplexing from dx-streaming-upload runs
- `AUTH_TOKEN=`: DNAnexus API token
- `SLACK_TOKEN=`: slack API token for [hermes](hermes-url), used for sending slack notifications


### High level config design

Tab separated file that maps egg codes used in sample names to the respective assay and associated low level JSON config file. There should be one line per assay in the format `egg-code assay file-id`.

Example lines in config file:
```
EGG1  TSOE  file-G46Zv404yz7f1fV230207Vpz
EGG2  myeloid file-G5FYKxQ469Fk9ykvFkbGfxQb
```

### Low level / assay config file

The assay config file for the conductor app is designed to be written as a JSON file, with each workflow or apps defined as an executable. For each workflow/app, there are several required and optional keys to add, with a required structure. An example empty template and populated config file may be found [here](example/config_template.json) and [here](example/example_populated_config.json) respectively.

As the config file is a JSON, several fields may be added to enhance readability that will not be parsed when running, such as the name, details and GitHub URL for each executable.

**Required keys in the top level of the config include**:

- `demultiplex` (boolean): if true, run bcl2fastq to generate fastqs
- `users` (dict): DNAnexus users to add to output project and access level to be granted
- `executables` (dict): each key should be the workflow or app id, with it's value being a dictionary (see below for example)

**Optional keys in top level of config include**:

- `sample_name_regex` (list): list of regex patterns to use for performing samplesheet validation on sample names with

Example top level of config:
```{
    "name": "Config for myeloid assay",
    "version": "v1.0.0",
    "details": "Includes main Uranus workflow, multi-fastqc and uranus annotation workflow",
    "demultiplex": true,
    "users": {
        "org-emee_1": "CONTRIBUTE"
    },
    "sample_name_regex": [
        "[0-9]{7}-[A-Z0-9]*-[A-Z]{2,3}-[A-Za-z]*-MYE-[FMNU]-EGG2",
        "Oncospan-[A-za-z0-9-]*"
    ]
```

**Required keys per executable dictionary**:

- `name`: will be used to name output directory if using output variable naming (see below)
- `analysis`: the value should be written as `analysis_1`, where the number is the executable stage in the config (i.e for the first workflow app this would be `analysis_1`, for the second `analysis_2`...). This is used to link the outputs of one workflow / app to subsequent workflows / apps.
- `per_sample` (boolean): if to run the executable on each sample individually, or as one job
- `process_fastqs` (boolean): if the executable requires fastqs passing
- `inputs` (dict): this forms the input dictionary passed to the call to dx api to trigger the running of the executable, more details may be found [here](dx-run-url). See below for structure and available inputs.
- `output_dirs` (dict): maps the app / workflow stages to directories in which to store output data. See below for structure and available inputs.

**Optional keys per executable dictionary**:

- `depends_on` (list): Where an executables input(s) are dependent on the output of a previous job(s), these should be defined as a list of strings. This relies on using the `analysis_X` key, where `X` is the number of the dependent executable to collect the output from
    - (e.g. `"output_dirs": ["analysis_1"]`, where the job is dependent on the first executable)
- `sample_name_delimeter` (str): string to split sample name on and pass to where `INPUT-SAMPLE-NAME` is used. Useful for passing as input where full sample name is not wanted (i.e. for displaying in report)
- `details` (str): not parsed by the app but useful for adding verbosity to the config when reading by humans
- `url` (str): same as above for `details`, just acts as a reference to GitHub provenance of what is being used for analysis


Example of per executable config:
```
    "executables": {
        "workflow-G4VpkG8433GZKf90KkXB4XZx": {
            "name": "uranus_main_workflow_GRCh38_v1.5.0_novaseq",
            "details": "Main Uranus workflow for alignment and variant calling",
            "url": "https://github.com/eastgenomics/eggd_uranus_main_workflow",
            "analysis": "analysis_1",
            "per_sample": true,
            "sample_name_delimeter": "_",
            "process_fastqs": true,
            "inputs": {
                "stage-G0qpXy0433Gv75XbPJ3xj8jV.reads_fastqgzs": "INPUT-R1",
                "stage-G0qpXy0433Gv75XbPJ3xj8jV.reads2_fastqgzs": "INPUT-R2"
            },
            "output_dirs": {
                "stage-G0qpXy0433Gv75XbPJ3xj8jV": "/output/OUT-FOLDER/APP-NAME",
                "stage-G0qpY1Q433GpzBp958KJYfBK": "/output/OUT-FOLDER/APP-NAME",
                "stage-G21GYKj4q5J37F0B5ky018QG": "/output/OUT-FOLDER/APP-NAME",
                "stage-G02ZFz0433GpQB4j9Gvg0b81": "/output/OUT-FOLDER/APP-NAME",
                "stage-G0Y87ZQ433Gy6y7vBB74p30j": "/output/OUT-FOLDER/APP-NAME",
                "stage-G0KbB6Q433GyV6vbJZKVYV96": "/output/OUT-FOLDER/APP-NAME",
                "stage-Fy0549Q41zg02Kjg05x69yvK": "/output/OUT-FOLDER/APP-NAME",
                "stage-Fv6jY9Q4KB7yKfKx8Fq8b7zG": "/output/OUT-FOLDER/APP-NAME",
                "stage-Fy4j4K041zgF5Z8y0x9KjV55": "/output/OUT-FOLDER/APP-NAME",
                "stage-G02ZG6Q433GV76v29b6Gggjp": "/output/OUT-FOLDER/APP-NAME"
            }
        },
```


### Structuring the inputs dictionary

The inputs dict may be given several inputs that act as placeholders to be parsed by the script at runtime. Each key value pair should be given as the app/stage input as the key, and the placeholder as the value. The key MUST match the input given in the specified workflow /apps available inputs (i.e. in `dxapp.json` for apps, `stage-id.input` for workflows). These are all prefixed with `INPUT-` to be identifiable.

Currently, these include the following:

- `INPUT-R1`: indicates to pass 1 or more R1 fastq files as input
- `INPUT-R2`: indicates to pass 1 or more R2 fastq files as input
- `INPUT-R1-R2`: indicates to pass all R1 AND R2 fastq files as input
- `INPUT-dx_project_id`: pass the project id used for analysis
- `INPUT-dx_project_name`: pass the project name used for analysis
- `INPUT-analysis_X-out_dir`: pass the output directory of analysis `X` as input, where `X` is the number of the analysis defined as above

Inputs dependent on the output of a previous job should be defined as shown below. This relies on using the `analysis_X` key, where `X` is the number of the executable to collect the output from.

n.b. where any inputs are linked to previous job outputs, the `depends_on` key should be given in the executable keys, with the value as a list of `analysis_X` to hold for the completion of that job.

- For workflows:

    ```
    "stage-G0QQ8jj433Gxyx2K8xfPyV7B.input_vcf": {
                        "$dnanexus_link": {
                            "analysis": "analysis_1",
                            "stage": "stage-G0Y87ZQ433Gy6y7vBB74p30j",
                            "field": "out"
                        }
                    },
     ```

- For apps / applets:

    ```
    "applet-G48VfX8433GypYP8418pPfq7.panel_bed": {
                        "$dnanexus_link": {
                            "analysis": "analysis_2",
                            "stage": "applet-G49452Q433GzX0jy3Gg60vz6",
                            "field": "bed_file"
                        }
                    },
    ```


### Structuring the output_dirs dictionary

This defines the output directory structure for the executables outputs. For workflows, each stage should have the `stage-id: /path_to_output/` defined. These may either be hardcoded strings, or optionally use either or both of the following 2 placefolders to subsitute:

- `OUT-FOLDER`: will be named with `/output/` and the `"name"` field for the executable
- 'APP-NAME': will use the name for the given stage / app id from a `dx describe` call

- For workflows:

    ```
    "output_dirs": {
        "stage-G0QQ8jj433Gxyx2K8xfPyV7B": "/output/OUT-FOLDER/APP-NAME",
        "stage-G0QQ8q8433Gb8YFG4q7847Px": "/output/OUT-FOLDER/APP-NAME"
    }
    ```

- For apps / applets:

    ```
    "output_dirs": {
        "applet-Fz93FfQ433Gvf6pKFZYbXZQf": "/output/APP-NAME"
    }

    ```


## Requirements

The app has various design patterns that have certain requirements. These include:

- The path specified to download the `eggd_conductor.cfg` and `high_level_config.tsv` assumes that these files will have a `version` and a `CAPA` property atrribute set. The `version` property should follow semantic versioning and the config file with the highest version will be used. The `CAPA` property indicates a config file has been signed off for use, only config files with this property will be used for analysis. The `properties` atrribute parsed from a `dx describe` call on one of these config files will look as such:
    ```
    $ dx describe --json project-G6F2k284Fgb5388bBq7fGZX7:file-G6ZxGQj4Fgb2k4Fk3j1VjKpk | jq -r '.properties'
    {
    "version": "1.0.0",
    "CAPA": "GEN.BI.123"
    }
    ```
- The matching of samples to assay config files relies on the use of an "`EGG`" code in the sample name. This naming convention used at East GLH for samples links a sample to an asaay (i.e samples with `EGG2` in the name indicates they are to be processed with the Uranus workflow). The use of these codes is currently not configurable.
- If a dx-streaming-upload sentinel file is being used and analysis is not starting from fastqs, then it is assumed that demultiplexing will be performed with the [bcl2fastq app](bcl2fastq-url) developed by East GLH. On instrument demultplexed data uploaded via dx-streaming-upload is not currently supported.


## App Inputs - Defaults & Behaviour

The following describe default app input behaviour:

- `DNAnexus project id`: Project in which to run and store output, if not specified will create a new project named as `002_<RUNID>_<ASSAY_CODE>` or `003_YYMMDD_<RUNID>_<ASSAY_CODE>` if `development=true`
- `Sentinel file`: sentinel file created by dx-streaming-upload to use for specifying run data for analysis
- `SampleSheet`: samplesheet used to parse sample names from, if not given this will be attempted to be located from the sentinel file run directory, or the first upload tar file.
- `sample names`: comma separated list of sample names, to use if not providing a samplesheet
- `fastqs`: array of fastq files, to use if not providing a sentinel file
- `high level config`: high level config file, overrides getting default from `high level path`
- `low level config`: use given low level config file for all sample analysis instead of inferring config to use from sample names
- `eggd conductor config`: config file to use for bcl2fastq app ID and API tokens, if given will override `eggd_conductor path`
- `assay type`: specify assay type to use a given assay, overrides parsing of EGG codes
- `validate samplesheet`: if to perform samplesheet validation. If validation fails but is an acceptable error the job may be re-run with `validate samplesheet = false`
- `development`: if set to `true` will name output project prefixed with `003` instead of `002`
- `bcl2fastq job id`: use output fastqs of a previous bcl2fastq job instead of performing demultiplexing
- `bcl2fastq output path`: where to store the output of bcl2fastq, defaults to the parent directory of the sentinel file
- `eggd_conductor path`: path to where `eggd_conductor.cfg` files are stored, if given will override the default
- `high level path`: path to where `high_level_config.tsv` files are stored, if given will override the default


## Dependencies

The following release `.tar.gz` are required to be included in `/resources/home/dnanexus/`:

- [hermes slack bot](hermes-url): used to send notifications of launched jobs and errors to slack channels
- [samplesheet validator](samplesheet-validator-url): used for validating samplesheets before running with bcl2fastq


[dx-streaming-upload-url]: https://github.com/dnanexus-rnd/dx-streaming-upload
[dx-run-url]: http://autodoc.dnanexus.com/bindings/python/current/dxpy_apps.html?highlight=run#dxpy.bindings.dxapplet.DXExecutable.run
[hermes-url]: https://github.com/eastgenomics/hermes
[samplesheet-validator-url]: https://github.com/eastgenomics/validate_sample_sheet
[bcl2fastq-url]: https://github.com/eastgenomics/eggd_bcl2fastq